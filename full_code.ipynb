{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHbvh0g8iPyX"
   },
   "source": [
    "## **CISC873_Assignment1**\n",
    "\n",
    "Mahsa Aghaeeaval (10177616)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8c-HR3GjMvb"
   },
   "source": [
    "# Loading Data / Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9Yaz7RjSd0u"
   },
   "outputs": [],
   "source": [
    "# list of code resources used\n",
    "\n",
    "# Steven Ding's Code for the assignment\n",
    "# https://www.kaggle.com/aeshen/the-secret-to-getting-the-second-date\n",
    "# https://www.kaggle.com/lucabasa/the-data-science-book-of-love\n",
    "# https://www.kaggle.com/evanmiller/pipelines-gridsearch-awesome-ml-pipelines\n",
    "# https://towardsdatascience.com/doing-xgboost-hyper-parameter-tuning-the-smart-way-part-1-of-2-f6d255a45dde\n",
    "# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ce1NNIT4iMd-"
   },
   "outputs": [],
   "source": [
    "# libriaries used are loaded\n",
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, RobustScaler, Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_predict, RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.combine import SMOTEENN \n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5O2KUl7jT1c"
   },
   "outputs": [],
   "source": [
    "# grab dataset from google drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# loading the train_new.csv data as a pandas dataframe\n",
    "df = pd.read_csv(\"/content/drive/My Drive/CISC873_Assignments/Assignment1/train.csv\")\n",
    "\n",
    "# loading the test_new.csv data as a pandas dataframe to be used later for kaggle prediction\n",
    "df_test_for_kaggle = pd.read_csv(\"/content/drive/My Drive/CISC873_Assignments/Assignment1/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOwZ3oDnd56t"
   },
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8LghuwBd_sD"
   },
   "outputs": [],
   "source": [
    "# peak into data\n",
    "\n",
    "# print first and last few rows of the dataset\n",
    "print(df.head())\n",
    "print(df.tail())\n",
    "\n",
    "# print all column headers + type\n",
    "print(df.info())\n",
    "\n",
    "# print all missing values usinbg isnull() function\n",
    "obj = df.isnull().sum()\n",
    "for key,value in obj.iteritems():\n",
    "    print(key,\",\",value)\n",
    "\n",
    "#NOTES: These statistics will show how the data is structured and what it cocntains and gives an idea how much 'fun' this is going to be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCgS8znqiEcv"
   },
   "outputs": [],
   "source": [
    "# features/ target features distribution check\n",
    "\n",
    "# checking the ditribution of features\n",
    "select_feature = df['match'] # replace 'rating' with any feature you want to check\n",
    "\n",
    "feature_distribution_hist = plt.hist(select_feature) # dist using his\n",
    "plt.show()\n",
    "\n",
    "feature_distribution_kde = sns.kdeplot(select_feature) # dist using kde\n",
    "plt.show()\n",
    "\n",
    "# stats summary of features\n",
    "print(round(df.describe().T))\n",
    "print(df.median())\n",
    "\n",
    "\n",
    "#NOTES: checking the distribution helps understand the feature in terms of its range. It will help see if the values are centered or scattered.\n",
    "#NOTES: I am checking with 2 graphs (kde and histogram) since some feature are better represented using one\n",
    "#NOTES: I included stats summary (mean) and median to mathematically check to see if any features are heavily + or - skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-M4mdJDAuAS3"
   },
   "outputs": [],
   "source": [
    "# dropping columsn with missing values\n",
    "df = df.loc[:, df.isnull().mean() < .5]\n",
    "\n",
    "\n",
    "#NOTE: can change treshhold to any and see what columns are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbjOEmIxuFeu"
   },
   "outputs": [],
   "source": [
    "# print all the data type for every column to see what mess we're dealing with\n",
    "obj = df.dtypes\n",
    "for key,value in obj.iteritems():\n",
    "    print(key,\",\",value)\n",
    "\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rqW2IW1xAKP"
   },
   "outputs": [],
   "source": [
    "# creating an object free data-frame (JUST FOR EDA, df will be used for pipline)\n",
    "no_object_df = df.drop(['field', 'undergra', 'zipcode', 'income', 'from', 'career'], axis=1)\n",
    "\n",
    "print(no_object_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qufyxa7Yyr4J"
   },
   "outputs": [],
   "source": [
    "# heatmap to check attribute correlation\n",
    "plt.subplots(figsize=(20,15))\n",
    "ax = plt.axes()\n",
    "ax.set_title(\"Correlation Heatmap\")\n",
    "corr = no_object_df.corr()\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLsFH6Hp9Y1R"
   },
   "outputs": [],
   "source": [
    "# looking into a relationship between gender and matches\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "g = sns.FacetGrid(no_object_df, col=\"gender\")\n",
    "g = g.map(plt.hist, \"match\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oL10hicxFVrn"
   },
   "outputs": [],
   "source": [
    "# look into race\n",
    "# assignining the number to their respective string for better EDA. \n",
    "\n",
    "df['race'] = df.race.map({1: 'Black', 2: 'White', 3: 'Hispanic', \n",
    "                          4: 'Asian', 6: 'Other'}).fillna(df.race)\n",
    "\n",
    "df.race.value_counts(dropna=False)\n",
    "\n",
    "# look into race by age\n",
    "ax = df[['race', 'age']].groupby('race').mean().plot(kind='bar', figsize=(12,5), legend=False,\n",
    "                                               title='Mean Age by Race',\n",
    "                                                   ylim=(24,28), color='rgbmy')\n",
    "ax.set_xticklabels(['Asian', 'Black', 'Hispanic',  'Other', 'White'], \n",
    "                   fontsize=12, rotation='horizontal')\n",
    "ax.set_xlabel('',fontsize=1)\n",
    "\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_x()+.155, i.get_height()+.05, \\\n",
    "            str(round((i.get_height()), 1)), fontsize=12)\n",
    "\n",
    "# NOTE: I found the documentation file for the dataset which is how I know what string to assign to each number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qSsq76EG-B9"
   },
   "outputs": [],
   "source": [
    "# look into fields of study of the participants\n",
    "df['field_cd'] = df.field_cd.map({1: 'Law', 2: 'Math', 3: 'Soc. Sc.', 4: 'Med. Sc.',\n",
    "                                 5: 'Eng.', 6: 'Journ.', 7: 'Hist.', 8: 'Econ', 9: 'Educ.',\n",
    "                                 10: 'Nat. Sc.', 11: 'Soc. Wr.', 12: 'Und.', 13: 'Pol. Sc.',\n",
    "                                 14: 'Film', 15: 'Arts', 16:'Lang.', 17: 'Arch.', 18: 'Oth.'}).fillna(df.field_cd)\n",
    "df.field_cd.value_counts(dropna=False)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize = (12,5))\n",
    "ax = sns.countplot(x=\"field_cd\", data=df)\n",
    "plt.title('Field of study', fontsize=18)\n",
    "\n",
    "# field of study by gender plot\n",
    "tmp = df[['gender', 'field_cd']].groupby(['field_cd', 'gender']).size().unstack().fillna(0)\n",
    "ax = tmp.plot(kind='bar', figsize=(12,6), stacked=True)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=12, rotation=45)\n",
    "\n",
    "ax.set_title('Field of study by gender', fontsize=18)\n",
    "ax.set_xlabel('',fontsize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DmS9gG72Iw2u"
   },
   "outputs": [],
   "source": [
    "# Is race important (is it a decided factor for match)?\n",
    "\n",
    "# creates histogram of race importance using the imprace column\n",
    "ax = df.imprace.hist(bins=10, figsize=(12,8))\n",
    "ax.set_title('How important is the race', fontsize=15)\n",
    "ax.set_xlabel('Importance',fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhoePS0KJsKd"
   },
   "outputs": [],
   "source": [
    "# Is religion important (is it a decided factor for match)?\n",
    "ax = df.imprelig.hist(bins=10, figsize=(12,8))\n",
    "ax.set_title('How important is the religion', fontsize=15)\n",
    "ax.set_xlabel('Importance',fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2lyuQ3dL_EA"
   },
   "outputs": [],
   "source": [
    "# looking into why people are doing this (goals)\n",
    "df['goal'] = df.goal.map({1: 'Fun', 2: 'Meet', 3: 'Date', \n",
    "                          4: 'Relationship', 5: 'IdidIt', 6: 'Other'}).fillna(df.goal)\n",
    "\n",
    "df.goal.value_counts(dropna=False)\n",
    "\n",
    "# NOTE: I found the documentation file for the dataset which is how I know what string to assign to each number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHCU3XvRNK6Y"
   },
   "outputs": [],
   "source": [
    "# looking into people's interests\n",
    "def many_hist(cols):\n",
    "    num = len(cols)\n",
    "    rows = int(num/2) + (num % 2 > 0)\n",
    "    fig, ax = plt.subplots(rows, 2, figsize=(15, 5 * (rows)))\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for feat in cols:\n",
    "        df[feat].hist(label=feat, ax=ax[i][j])\n",
    "        ax[i][j].set_title(feat, fontsize=12)\n",
    "        ax[i][j].grid(False)\n",
    "        j = (j+1)%2\n",
    "        i = i + 1 - j\n",
    "# hists for interests\n",
    "interests = ['sports', 'tvsports', 'exercise', 'dining', 'museums',\n",
    "       'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater',\n",
    "       'movies', 'concerts', 'music', 'shopping', 'yoga']\n",
    "many_hist(interests)\n",
    "\n",
    "\n",
    "# heatmap for interests\n",
    "corr = df[interests].corr()\n",
    "plt.figure(figsize=(12,10))\n",
    "ax = sns.heatmap(corr, cmap='RdBu_r')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "ax.set_title('Correlation between interests', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "re5mR0BQHl-4"
   },
   "source": [
    "# Data preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IWTZYzHSuFfR"
   },
   "outputs": [],
   "source": [
    "# x and y for PIPELINE 1\n",
    "x = df.drop('match', axis=1)\n",
    "features_numeric = list(x.select_dtypes(include=['float64']))\n",
    "features_categorical = list(x.select_dtypes(include=['object']))\n",
    "y = df['match']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F73_n2wwHvy4"
   },
   "source": [
    "# Pipelines\n",
    "\n",
    "Since it would take too unnecassary space to have a cell for every variation of the model, I will make show all variations of my XGBClassifier (since it was my best model) and only one cell for every model (using grid search). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kA3bGt2j07_9"
   },
   "outputs": [],
   "source": [
    "# Pipeline 1 (BASE LINE) grid search + XGclassifier \n",
    "\n",
    "transformer_numeric = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())]\n",
    ")\n",
    "\n",
    "transformer_categorical = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', transformer_numeric, features_numeric),\n",
    "        ('cat', transformer_categorical, features_categorical)\n",
    "    ]\n",
    ")\n",
    "\n",
    "full_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_classifier', XGBClassifier(\n",
    "            objective='binary:logistic', seed=1))\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean'],\n",
    "    'my_classifier__n_estimators': [5, 20, 100, 500],\n",
    "    'my_classifier__max_depth':[3, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    full_pipline, param_grid, cv=5, verbose=3, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "grid_search.fit(x, y)\n",
    "\n",
    "print('best score {}'.format(grid_search.best_score_))\n",
    "print('best score {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTYHUttRJLHB"
   },
   "outputs": [],
   "source": [
    "# Pipeline 2 (MODIFIED) grid search + XGclassifier \n",
    "\n",
    "transformer_numeric = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())]\n",
    ")\n",
    "\n",
    "transformer_categorical = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', transformer_numeric, features_numeric),\n",
    "        ('cat', transformer_categorical, features_categorical)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# running SMOTE to overcome unbalanced data problem\n",
    "smote_enn = SMOTEENN()\n",
    "\n",
    "# running for k fold validation\n",
    "kf = StratifiedKFold()\n",
    "\n",
    "# conducting preprocessor the same way as pipeline, then adding TruncatedSVD() as well\n",
    "estimators = [('preprocessor', preprocessor), ('smote_enn', smote_enn), ('reduce_dim', TruncatedSVD()), ('my_classifier', XGBClassifier(objective='binary:logistic'))]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# `__` denotes attribute \n",
    "# (e.g. my_classifier__n_estimators means the `n_estimators` param for `my_classifier`\n",
    "#  which is our xgb)\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean','median','most_frequent'],\n",
    "    'reduce_dim__algorithm' : ['randomized','arpack'],\n",
    "    'my_classifier__n_estimators': [5, 20, 100, 500],\n",
    "    'my_classifier__max_depth':[3, 5, 10],\n",
    "    'my_classifier__min_child_weight': [0.01, 0.1 , 1],\n",
    "    'my_classifier__subsample' : [0.8],\n",
    "    'my_classifier__gamma' : [0.01, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipe, param_grid, cv= kf, verbose=3, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "grid_search.fit(x, y)\n",
    "\n",
    "print('best score {}'.format(grid_search.best_score_))\n",
    "print('best score {}'.format(grid_search.best_params_))\n",
    "\n",
    "\n",
    "## VERY IMPORTANT NOTE: Since the previous pipline i have addded SMOTE, kf validation, TruncatedSVD() and added more hyperparameter HOWEVER, these steps were NOT run all at once together.\n",
    "## I ran these functions one by one to observe model performance but wanted to keep my final code clean. Keep in mind to slighly change the pipeline structure if you're trying to run SMOTEEN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lETfwxoQO51C"
   },
   "outputs": [],
   "source": [
    "# Pipeline 2 (MODIFIED) Bayes search + XGclassifier \n",
    "\n",
    "transformer_numeric = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())]\n",
    ")\n",
    "\n",
    "transformer_categorical = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', transformer_numeric, features_numeric),\n",
    "        ('cat', transformer_categorical, features_categorical)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# running SMOTE to overcome unbalanced data problem\n",
    "smote_enn = SMOTEENN()\n",
    "\n",
    "# running for k fold validation\n",
    "kf = StratifiedKFold()\n",
    "\n",
    "# conducting preprocessor the same way as pipeline, then adding TruncatedSVD() as well\n",
    "estimators = [('preprocessor', preprocessor), ('smote_enn', smote_enn), ('reduce_dim', TruncatedSVD()), ('my_classifier', XGBClassifier(objective='binary:logistic'))]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "\n",
    "# `__` denotes attribute \n",
    "# (e.g. my_classifier__n_estimators means the `n_estimators` param for `my_classifier`\n",
    "#  which is our xgb)\n",
    "bayes_search = BayesSearchCV(\n",
    "    pipe,\n",
    "    {\n",
    "        'preprocessor__num__imputer__strategy': ['mean','median','most_frequent'],\n",
    "        'reduce_dim__algorithm' : ['randomized','arpack'],\n",
    "        'my_classifier__n_estimators': [5, 20, 100, 500],\n",
    "        'my_classifier__max_depth':[3, 5, 10],\n",
    "        'my_classifier__min_child_weight': [0.01, 0.1 , 1],\n",
    "        'my_classifier__subsample' : [0.8],\n",
    "        'my_classifier__gamma' : [0.01, 0.1, 0.5]\n",
    "    },\n",
    "    n_iter=3,\n",
    "    random_state=0,\n",
    "    verbose=3,\n",
    ")\n",
    "\n",
    "bayes_search.fit(x, y)\n",
    "\n",
    "print('best score {}'.format(grid_search.best_score_))\n",
    "print('best score {}'.format(grid_search.best_params_))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bayes_search = BayesSearchCV(\n",
    "    full_pipline,\n",
    "    {\n",
    "        'preprocessor__num__imputer__strategy': ['mean','median','most_frequent'],\n",
    "        'reduce_dim__algorithm' : ['randomized','arpack'],\n",
    "        'my_classifier__n_estimators': [5, 20, 100, 500],\n",
    "        'my_classifier__max_depth':[3, 5, 10],\n",
    "        'my_classifier__min_child_weight': [0.01, 0.1 , 1],\n",
    "        'my_classifier__subsample' : [0.8],\n",
    "        'my_classifier__gamma' : [0.01, 0.1, 0.5]\n",
    "    },\n",
    "    n_iter=3,\n",
    "    random_state=0,\n",
    "    verbose=3,\n",
    ")\n",
    "\n",
    "bayes_search.fit(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCouq_ohO9YJ"
   },
   "outputs": [],
   "source": [
    "# Pipeline 2 (MODIFIED) Random search + XGclassifier \n",
    "\n",
    "transformer_numeric = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())]\n",
    ")\n",
    "\n",
    "transformer_categorical = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', transformer_numeric, features_numeric),\n",
    "        ('cat', transformer_categorical, features_categorical)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# running SMOTE to overcome unbalanced data problem\n",
    "smote_enn = SMOTEENN()\n",
    "\n",
    "# running for k fold validation\n",
    "kf = StratifiedKFold()\n",
    "\n",
    "# conducting preprocessor the same way as pipeline, then adding TruncatedSVD() as well\n",
    "estimators = [('preprocessor', preprocessor), ('smote_enn', smote_enn), ('reduce_dim', TruncatedSVD()), ('my_classifier', XGBClassifier(objective='binary:logistic'))]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# `__` denotes attribute \n",
    "# (e.g. my_classifier__n_estimators means the `n_estimators` param for `my_classifier`\n",
    "#  which is our xgb)\n",
    "random_search = RandomizedSearchCV(\n",
    "    full_pipline,\n",
    "    {\n",
    "        'preprocessor__num__imputer__strategy': ['mean','median','most_frequent'],\n",
    "        'reduce_dim__algorithm' : ['randomized','arpack'],\n",
    "        'my_classifier__n_estimators': [5, 20, 100, 500],\n",
    "        'my_classifier__max_depth':[3, 5, 10],\n",
    "        'my_classifier__min_child_weight': [0.01, 0.1 , 1],\n",
    "        'my_classifier__subsample' : [0.8],\n",
    "        'my_classifier__gamma' : [0.01, 0.1, 0.5]\n",
    "    },\n",
    "    n_iter=3,\n",
    "    random_state=0,\n",
    "    verbose=3,\n",
    ")\n",
    "\n",
    "random_search.fit(x, y)\n",
    "\n",
    "print('best score {}'.format(grid_search.best_score_))\n",
    "print('best score {}'.format(grid_search.best_params_))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbZ-PgZKQfD4"
   },
   "source": [
    "I will now show only ONE example of each model with GRID search (best performing search) to let you see what parameter were used and whatnot. Obviously Pipeline 2 was also ran for these models but code will be ommitted as nothing really changes except for the hyperparameters (discussed more in detail in my document). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyYu4XFl4K8j"
   },
   "outputs": [],
   "source": [
    "# Pipeline 1 (grid search) + Random forest\n",
    "\n",
    "transformer_numeric = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())]\n",
    ")\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', transformer_numeric, features_numeric)]\n",
    ")\n",
    "\n",
    "\n",
    "full_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_classifier', RandomForestClassifier())])\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean'],\n",
    "    'my_classifier__n_estimators': [500, 700],\n",
    "    'my_classifier__max_depth':[10],\n",
    "    'my_classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'my_classifier__max_features': ['auto', 'sqrt'],\n",
    "    'my_classifier__min_samples_split': [2, 5, 10]\n",
    "\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    full_pipline, param_grid, cv=5, verbose=3, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "grid_search.fit(x, y)\n",
    "\n",
    "print('best score {}'.format(grid_search.best_score_))\n",
    "print('best score {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9y_LZ0hWarA"
   },
   "outputs": [],
   "source": [
    "# Pipeline 1 (grid search) + Logistic regression\n",
    "transformer_numeric = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())]\n",
    ")\n",
    "\n",
    "transformer_categorical = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', transformer_numeric, features_numeric),\n",
    "        ('cat', transformer_categorical, features_categorical)\n",
    "    ]\n",
    ")\n",
    "\n",
    "full_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_classifier', LogisticRegression(class_weight='balanced'))])\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean'],\n",
    "    'my_classifier__C':np.logspace(-3,3,7),\n",
    "    'my_classifier__penalty':['l1','l2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    full_pipline, param_grid, cv=5, verbose=3, n_jobs=-1, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "grid_search.fit(x, y)\n",
    "\n",
    "print('best score {}'.format(grid_search.best_score_))\n",
    "print('best score {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAFCYCRnuRwl"
   },
   "outputs": [],
   "source": [
    "# Pipeline 1 (grid search) + SVC\n",
    "transformer_numeric = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())]\n",
    ")\n",
    "\n",
    "transformer_categorical = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', transformer_numeric, features_numeric),\n",
    "        ('cat', transformer_categorical, features_categorical)\n",
    "    ]\n",
    ")\n",
    "\n",
    "full_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_classifier', SVC(class_weight='balanced'))])\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'my_classifier__kernel': ['rbf','poly'], \n",
    "    'my_classifier__gamma': [1e-3, 1e-4],\n",
    "    'my_classifier__C': [1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    full_pipline, param_grid, cv=5, verbose=3, n_jobs=-1, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "grid_search.fit(x, y)\n",
    "\n",
    "print('best score {}'.format(grid_search.best_score_))\n",
    "print('best score {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mw5y6HTKoZwL"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Pipeline 1 (grid search) + SVC\n",
    "transformer_numeric = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())]\n",
    ")\n",
    "\n",
    "transformer_categorical = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', transformer_numeric, features_numeric),\n",
    "        ('cat', transformer_categorical, features_categorical)\n",
    "    ]\n",
    ")\n",
    "\n",
    "full_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_classifier', MLPClassifier())])\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'my_classifier__hidden_layer_sizes': [(50,100,50), (100,)],\n",
    "    'my_classifier__activation': ['tanh', 'relu'],\n",
    "    'my_classifier__solver': ['sgd', 'adam'],\n",
    "    'my_classifier__alpha': [0.0001, 0.05],\n",
    "    'my_classifier__learning_rate': ['constant','adaptive']\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    full_pipline, param_grid, cv=4, verbose=3, n_jobs=-1, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "grid_search.fit(x, y)\n",
    "\n",
    "print('best score {}'.format(grid_search.best_score_))\n",
    "print('best score {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cW7UcidHzuSh"
   },
   "source": [
    "# Predict and save as csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygnlmRXDRgDT"
   },
   "source": [
    "Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6M9BvbVWqcA"
   },
   "outputs": [],
   "source": [
    "# prepare submission for grid_search:\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = df_test_for_kaggle['id']\n",
    "submission['match'] = grid_search.predict_proba(df_test_for_kaggle)[:,1]\n",
    "submission.to_csv('name_file_here.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puawOeKDRlPD"
   },
   "source": [
    "Bayes search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNoXzcA-79f-"
   },
   "outputs": [],
   "source": [
    "# prepare submission for bayes_search:\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = df_test_for_kaggle['id']\n",
    "submission['match'] = bayes_search.predict_proba(df_test_for_kaggle)[:,1]\n",
    "submission.to_csv('name_file_here.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-LFW2p1Rq6U"
   },
   "source": [
    "Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UsXvpNQzmyo"
   },
   "outputs": [],
   "source": [
    "# prepare submission for random_search:\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = df_test_for_kaggle['id']\n",
    "submission['match'] = random_search.predict_proba(df_test_for_kaggle)[:,1]\n",
    "submission.to_csv('name_file_here.csv', index=False)\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
