{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CISC873_Assign0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDuzZDXsTBHt"
      },
      "source": [
        "# Problem Definition\n",
        "\n",
        "Predicting product rating from Wish.com dataset. I will start by emplementing data exploration techniques to evalute the nature and significant of given features. I will pre-process the data to only contain valuable information used in model prediction. Several models will be hypertuned and compared based on their F-score (mean F-score). \n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Loading data / libraries\n",
        "\n",
        "2. Data exploration\n",
        "\n",
        "3. Data pre-processing\n",
        "\n",
        "4. Normalizing / Scaling / Encoding / Feature generation\n",
        "\n",
        "5. Final data preparation\n",
        "\n",
        "6. Modeling and performance evaluation\n",
        "\n",
        "7. Feature evaluation\n",
        "\n",
        "8. Findings\n",
        "\n",
        "9. Predict rating for test_new.csv to be sumbitted on Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_8Ebob-U44P"
      },
      "source": [
        "# Loading Data / Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZkaW0S0bYSu"
      },
      "source": [
        "# list of code resources used\n",
        "\n",
        "# https://www.kaggle.com/nibukdk93/summer-sales-wish-acc-100\n",
        "# https://www.kaggle.com/niteshhalai/sales-of-summer-clothes-in-e-commerce-wish\n",
        "# https://www.kaggle.com/mudithsilva/unit-sold-predict-model\n",
        "# https://www.kaggle.com/tanujdhiman/summer-product-sale-rating"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR6fxWY8XbuQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "472aa8dd-7b15-45ed-9fa9-4879025727a0"
      },
      "source": [
        "# libriaries used are loaded\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from statistics import mean, stdev\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import KFold,train_test_split,cross_val_score\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import VotingClassifier"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_TI3JdhSXbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ddaf381-eff1-4f4b-b11c-cd88113f6390"
      },
      "source": [
        "# grab dataset from google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# loading the train_new.csv data as a pandas dataframe\n",
        "df = pd.read_csv(\"/content/drive/My Drive/CISC873_Assignment0_data/train_new.csv\")\n",
        "\n",
        "# loading the test_new.csv data as a pandas dataframe to be used later for kaggle prediction\n",
        "df_test_for_kaggle = pd.read_csv(\"/content/drive/My Drive/CISC873_Assignment0_data/test_new.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa_ozanxPJVb"
      },
      "source": [
        "# grabbing the IDs to be later used in predicting using x_test dataset\n",
        "df_test_id = pd.read_csv(\"/content/drive/My Drive/CISC873_Assignment0_data/test_new.csv\")\n",
        "test_id=df_test_id['id']\n",
        "print(test_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRbqzv56WpXV"
      },
      "source": [
        "# Data exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACJqW_84W15r"
      },
      "source": [
        "# peak into data\n",
        "\n",
        "# print first and last few rows of the dataset\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "\n",
        "# print all column headers + type\n",
        "print(df.info())\n",
        "\n",
        "# checking for missing values for every feature\n",
        "df.isnull().sum()\n",
        "\n",
        "#NOTES: These statistics will show how the data is structured and what it cocntains and gives an idea how much 'fun' this is going to be"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSsVy5i9Y0Y7"
      },
      "source": [
        "# synonym features check\n",
        "\n",
        "# heat correlation map to check to for synonym columsn (identical or almost identical faetures)\n",
        "plt.figure(figsize = (20, 10))\n",
        "sns.heatmap(df.corr(), annot = True, cmap = 'coolwarm', center = 0)\n",
        "plt.show()\n",
        "\n",
        "#NOTES: heat map correlation map can be used to identify near identical features (columns) if they are highly correlated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-mkDYUGZXrz"
      },
      "source": [
        "# uni-value features chck\n",
        "\n",
        "# check to for uni-value columns\n",
        "def is_unique(s):\n",
        "    a = s.to_numpy()\n",
        "    return (a[0] == a).all()\n",
        "# returns True if all values of a feature are the same\n",
        "is_unique(df['currency_buyer']) # replace 'currency_buyer' with any feature you want to check\n",
        "\n",
        "#NOTES: checking for features that have the same or mostly the same values for every row as they are not descriptive enough "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yqg2hAjWbDNo"
      },
      "source": [
        "# features/ target features distribution check\n",
        "\n",
        "# checking the ditribution of features\n",
        "select_feature = df['rating'] # replace 'rating' with any feature you want to check\n",
        "\n",
        "feature_distribution_hist = plt.hist(select_feature) # dist using his\n",
        "plt.show()\n",
        "\n",
        "feature_distribution_kde = sns.kdeplot(select_feature) # dist using kde\n",
        "plt.show()\n",
        "\n",
        "# stats summary of features\n",
        "print(round(df.describe().T))\n",
        "print(df.median())\n",
        "\n",
        "\n",
        "#NOTES: checking the distribution helps understand the feature in terms of its range. It will help see if the values are centered or scattered.\n",
        "#NOTES: I am checking with 2 graphs (kde and histogram) since some feature are better represented using one\n",
        "#NOTES: I included stats summary (mean) and median to mathematically check to see if any features are heavily + or - skewed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzFDg2wFLh_0"
      },
      "source": [
        "# checking for significant relationships between features/features and features/target\n",
        "\n",
        "\n",
        "# to see if there is a relantionship between features and target first I have determine what is considered a good rating and section off those products as is_successful_rating\n",
        "# coverting to int since it was all .0 anyway\n",
        "df.rating = df.rating.astype(int)\n",
        "def is_successful_rating(rating):\n",
        "    if rating >= 4:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# creating the column containing those only with ratings >= 4 and adds to df\n",
        "df['is_successful_rating'] = df['rating'].apply(is_successful_rating)\n",
        "\n",
        "# Check the difference between price and retail_price and see if there an association between those compared to successful vs unsucessful products\n",
        "print('Overall stats:')\n",
        "print(df['price'].mean())\n",
        "print(df['retail_price'].mean())\n",
        "print('----------------------')\n",
        "print('Stats for successful products:')\n",
        "print(df[df['is_successful_rating'] == 1]['price'].mean())\n",
        "print(df[df['is_successful_rating'] == 1]['retail_price'].mean())\n",
        "print('----------------------')\n",
        "print('Stats for unsuccessful products:')\n",
        "print(df[df['is_successful_rating'] == 0]['price'].mean())\n",
        "print(df[df['is_successful_rating'] == 0]['retail_price'].mean())\n",
        "\n",
        "# checking to see if difference in price between retail and price is associated with ratings\n",
        "df['diff_in_price'] = round(df['retail_price'] - df['price'],2)\n",
        "sns.violinplot(data=df, y='diff_in_price', x='is_successful_rating') # y (for sns plot) was changed to various features to see the results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3Xz6C3wEsOu"
      },
      "source": [
        "# Data cleaning and pre-processing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74STcqI2FiGo"
      },
      "source": [
        "# fixing some of the feature formatting for uniformity \n",
        "\n",
        "# fixing 'has_urgency_banner'\n",
        "# has_urgency_banner needs to be converted binomial so here I am filling 0 for the missing values\n",
        "df['has_urgency_banner'] = df['has_urgency_banner'].fillna(0)\n",
        "# has_urgency_banner needs to be converted binomial so here I am converting to int to match other binary columns\n",
        "df.has_urgency_banner = df.has_urgency_banner.astype(int)\n",
        "\n",
        "# fixing 'urgency_text'\n",
        "# has_urgency_banner needs to be converted binomial so here I am filling 0 for the missing values\n",
        "df['urgency_text'] = df['urgency_text'].fillna(0)\n",
        "# convert string to int to change uantité limitée ! to NaN\n",
        "df.urgency_text = pd.to_numeric(df.urgency_text, errors='coerce')\n",
        "# Nan to 1 to create binary field\n",
        "df['urgency_text'] = df['urgency_text'].fillna(1)\n",
        "# has_urgency_banner needs to be converted binomial so here I am converting to int to match other binary columns\n",
        "df.urgency_text = df.urgency_text.astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbVdMtetHZBL"
      },
      "source": [
        "# unifying different variations of the same thing under one category e.g blue and Blue and light blue = blue\n",
        "\n",
        "# Since this feature consists mainly of US and China we can combine the rest into others\n",
        "df['origin_country'] = df['origin_country'].replace('VE', 'Other')\n",
        "df['origin_country'] = df['origin_country'].replace('AT', 'Other')\n",
        "df['origin_country'] = df['origin_country'].replace('SG', 'Other')\n",
        "df['origin_country'] = df['origin_country'].replace('GB', 'Other')\n",
        "df['origin_country'] = df['origin_country'].replace(np.nan, 'Other')\n",
        "# visual check to see if it correctly categorized everything\n",
        "sns.countplot('origin_country', data=df)\n",
        "\n",
        "\n",
        "\n",
        "# Replacing different variations of sizes into one\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace('S.', 'S')\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace('XS.', 'S')\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace('M.', 'M')\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace('Size S', 'S')\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace('Size-XS', 'S')\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace('SIZE XS', 'S')\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace('Size-S', 'S')\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace('Size4XL', 'XL')\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace('size S', 'S')\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace('Size M', 'M')\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace('Size -XXS', 'S')\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace('SIZE-XXS', 'S')\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace('Size S.', 'S')\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace('s', 'S')\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace('SizeL', 'L')\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace('5XL', 'XL')\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace('4XL', 'XL')\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace('3XL', 'XL')\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace('2XL', 'XL')\n",
        "\n",
        "# list anything != name to OTHER to account for other variations\n",
        "def pr_var(name):\n",
        "    if name == 'XXXS' \\\n",
        "    or name == 'XXS' \\\n",
        "    or name == 'XS' \\\n",
        "    or name == 'S' \\\n",
        "    or name == 'M' \\\n",
        "    or name == 'L' \\\n",
        "    or name == 'XL' \\\n",
        "    or name == 'XXL' \\\n",
        "    or name == 'XXXXL' \\\n",
        "    or name == 'XXXXXL':\n",
        "        return name\n",
        "    else:\n",
        "        return \"OTHER\"\n",
        "\n",
        "# replace missing values with OTHER \n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].replace(np.nan, 'OTHER')\n",
        "# adding the new categories to df['feature']\n",
        "df['product_variation_size_id'] = df['product_variation_size_id'].apply(pr_var)\n",
        "\n",
        "\n",
        "# graph to look at size distribution to see if it correctly categorized everything\n",
        "fig_dims = (10, 10)\n",
        "fig, ax = plt.subplots(figsize=fig_dims)\n",
        "sns.countplot('product_variation_size_id',\n",
        "              order = df['product_variation_size_id'].value_counts().index,\n",
        "              palette=\"magma\",\n",
        "              data = df,\n",
        "              ax = ax)\n",
        "ax.set(xlabel='Size', ylabel='Count')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Replacing different variations of colors into one and change missing values to OTHER\n",
        "\n",
        "df['product_color'] = df['product_color'].replace('Black', 'black')\n",
        "df['product_color'] = df['product_color'].replace('White', 'white')\n",
        "df['product_color'] = df['product_color'].replace('navyblue', 'blue')\n",
        "df['product_color'] = df['product_color'].replace('lightblue', 'blue')\n",
        "df['product_color'] = df['product_color'].replace('skyblue', 'blue')\n",
        "df['product_color'] = df['product_color'].replace('darkblue', 'blue')\n",
        "df['product_color'] = df['product_color'].replace('navy', 'blue')\n",
        "df['product_color'] = df['product_color'].replace('winered', 'red')\n",
        "df['product_color'] = df['product_color'].replace('rosered', 'red')\n",
        "df['product_color'] = df['product_color'].replace('rose', 'red')\n",
        "df['product_color'] = df['product_color'].replace('orange-red', 'red')\n",
        "df['product_color'] = df['product_color'].replace('lightpink', 'pink')\n",
        "df['product_color'] = df['product_color'].replace('armygreen', 'green')\n",
        "df['product_color'] = df['product_color'].replace('khaki', 'green')\n",
        "df['product_color'] = df['product_color'].replace('lightgreen', 'green')\n",
        "df['product_color'] = df['product_color'].replace('fluorescentgreen', 'green')\n",
        "df['product_color'] = df['product_color'].replace('gray', 'grey')\n",
        "df['product_color'] = df['product_color'].replace('coffee', 'brown')\n",
        "df['product_color'] = df['product_color'].replace('multicolor', 'other')\n",
        "df['product_color'] = df['product_color'].replace('floral', 'other')\n",
        "df['product_color'] = df['product_color'].replace('leopard', 'other')\n",
        "df['product_color'] = df['product_color'].replace('camouflage', 'other')\n",
        "df['product_color'] = df['product_color'].replace('white & green', 'dual')\n",
        "df['product_color'] = df['product_color'].replace('black & green', 'dual')\n",
        "df['product_color'] = df['product_color'].replace('black & white', 'dual')\n",
        "df['product_color'] = df['product_color'].replace('camouflage', 'other')\n",
        "df['product_color'] = df['product_color'].replace(np.nan, 'other')\n",
        "\n",
        "# graph to look at size distribution to see if it correctly categorized everything\n",
        "fig_dims = (10, 15)\n",
        "fig, ax = plt.subplots(figsize=fig_dims)\n",
        "sns.countplot('product_color',\n",
        "              data = df,\n",
        "              order = df['product_color'].value_counts().iloc[:15].index,\n",
        "              ax = ax)\n",
        "ax.set(xlabel='Product Colour', ylabel='Count')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# checking for distribution of units sold \n",
        "df['units_sold'].value_counts()\n",
        "\n",
        "# we have some units less than 10 so we're going to group them\n",
        "def below_ten(units_sold):\n",
        "    if units_sold < 10:\n",
        "        return 10\n",
        "    else:\n",
        "        return units_sold\n",
        "\n",
        "df['units_sold'] = df['units_sold'].apply(below_ten)\n",
        "# checking for distribution of units sold to see if it correctly categorized everything\n",
        "df['units_sold'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVWns-OJRVGS"
      },
      "source": [
        "# checking for duplicates in df\n",
        "df.duplicated().sum()\n",
        "\n",
        "# removing duplicates\n",
        "df.drop_duplicates(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Msp7KxoYRBv-"
      },
      "source": [
        "# drop unneeded columns from dataset\n",
        "df.drop(['currency_buyer', 'theme','crawl_month', 'shipping_option_name','inventory_total','merchant_name','merchant_title','merchant_info_subtitle','id','merchant_profile_picture','urgency_text','is_successful_rating','countries_shipped_to','product_color','product_variation_size_id','merchant_id','shipping_is_express','badge_local_product'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "# NOTE: very dynamic column. features were added and removed to optimize accuracy. For example if \"product_color\" was removed here then it cannot be normalzied in the next section"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiYWMpiDPuBe"
      },
      "source": [
        "# Normalizing / scaling / encoding features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8BILNv8P_Uo"
      },
      "source": [
        "# normalizing feature to prepare data for model prediction (for color)\n",
        "df = pd.get_dummies(df, \n",
        "                    columns = ['product_color'],\n",
        "                    prefix = 'color_',\n",
        "                    drop_first = True)\n",
        "# quick check to see if it worked\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS1ghjCUQO8q"
      },
      "source": [
        "# normalizing feature to prepare data for model prediction (for size)\n",
        "df = pd.get_dummies(df, \n",
        "                    columns = ['product_variation_size_id'],\n",
        "                    prefix = 'size_',\n",
        "                    drop_first = True)\n",
        "# quick check to see if it worked\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX6M8VNvQW95"
      },
      "source": [
        "# normalizing feature to prepare data for model prediction (for origin_country)\n",
        "df = pd.get_dummies(df, columns = ['origin_country'],\n",
        "                    prefix = 'country_')\n",
        "# quick check to see if it worked\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDiEBxkYQkvd"
      },
      "source": [
        "# feature manipulation\n",
        "# breaking up the tag column to see the number of tags per row. \n",
        "\n",
        "def tag_count(tags):\n",
        "    tag_str = tags\n",
        "    # seperate words by commas\n",
        "    prod_tags = tag_str.split(',')\n",
        "    return len(prod_tags)\n",
        "    \n",
        "# replaced tags with tag_count from above to the df\n",
        "df['tag_count'] = df['tags'].apply(tag_count)\n",
        "df.drop(['tags'], axis=1, inplace=True)\n",
        "\n",
        "# NOTES: this is done bacause tags contains all strings which cannot be processed by models so this is one way to encode it. \n",
        "# NOTES: if you don't want to create \"tag_count\" make sure to remove \"tags\" in the drop columns cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zgeZJuPjdsb"
      },
      "source": [
        "# FEATURE ENGINEERING : comparing rating and rating count\n",
        "# Run only if you want to add this feature to df\n",
        "def new_feature(rating_count):\n",
        "  # assigning mean and standard deviation to variables to be used in the conditionals\n",
        "  mean_count = mean(df['rating_count'])\n",
        "  stdev_count = stdev(df['rating_count'])\n",
        "\n",
        "  if int(rating_count) >= mean_count + stdev_count:\n",
        "    return 2\n",
        "  elif int(rating_count) >= mean_count - stdev_count:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "df['new_feature_rating'] = df[['rating_count']].apply(new_feature, axis=1)\n",
        "\n",
        "# NOTES: this feature helps identify those that have high-rating/low-rating-count and vice versa. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2uhrkBx2QZM"
      },
      "source": [
        "print(df['new_feature_rating'].to_numpy()[0:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7nA6geY1yU7"
      },
      "source": [
        "# FEATURE ENGINEERING : comparing rating and rating count\n",
        "# Run only if you want to add this feature to df\n",
        "def new_feature(merchant_rating_count):\n",
        "\n",
        "  mean_count = mean(df['merchant_rating_count'])\n",
        "  stdev_count = stdev(df['merchant_rating_count'])\n",
        "\n",
        "  if int(merchant_rating_count) >= mean_count + stdev_count:\n",
        "    return 2\n",
        "  elif int(merchant_rating_count) >= mean_count - stdev_count:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "df['new_feature_merchant_rating'] = df[['merchant_rating_count']].apply(new_feature, axis=1)\n",
        "\n",
        "# NOTES: this feature helps identify those that have high-rating/low-rating-count and vice versa. Same feature as previous cell but this is for merchants not products"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elzCx734R6He"
      },
      "source": [
        "# CAUTION: only run if you want to normalize values\n",
        "# MinMaxScaler scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one, where min, max = feature_range.\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(df)\n",
        "df_normalized=pd.DataFrame(x_scaled, columns=df.columns)\n",
        "\n",
        "# NOTES: normalizes numerical values and saved them to a new df called \"df_normalized\" so we don't lose the not-normlized values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5oS--iDRj83"
      },
      "source": [
        "# quick check before modeling\n",
        "\n",
        "# View info un-normalized dataset\n",
        "print(df)\n",
        "print(df.info())\n",
        "\n",
        "# View info normalized dataset\n",
        "print(df_normalized)\n",
        "print(df_normalized.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANXMpLeKVVaz"
      },
      "source": [
        "# Final data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8d863TtifdF"
      },
      "source": [
        "Cross validation dataset \n",
        "\n",
        "*only run one*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjCuIEPtiumR"
      },
      "source": [
        "# training x and y without splitting to be used for cross validation if needed (FOR UN-NORMALIZED)\n",
        "X_no_split = df.drop(['rating'], axis = 1)\n",
        "y_no_split = df['rating']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFpmrrZ64xk7"
      },
      "source": [
        "# training x and y without splitting to be used for cross validation if needed (FOR NORMALIZED)\n",
        "X_no_split = df_normalized.drop(['rating'], axis = 1)\n",
        "y_no_split = df['rating']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIYke-pminD9"
      },
      "source": [
        "Un-normalized and normalized dataset \n",
        "\n",
        "*only run one*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNO91gafVUpS"
      },
      "source": [
        "# X/y train/test creation UN-NORMALIZED (only run this or NORMALIZED)\n",
        "\n",
        "X_not_nor = df.drop(['rating'], axis = 1)\n",
        "y_not_nor = df['rating']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_not_nor, y_not_nor, \n",
        "                                                    test_size = 0.2)\n",
        "# NOTE : uses df\n",
        "# NOTE : seed was set when optimizing models but I removed it after"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heV6phUsWMEt"
      },
      "source": [
        "# X/y train/test creation for NORMALIZED (only run this or UN-NORMALIZED)\n",
        "\n",
        "\n",
        "X_nor = df_normalized.drop(['rating'], axis = 1)\n",
        "# we don't want a normalized target attribute since this is a classification problem\n",
        "y_nor = df['rating']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_nor, y_nor, \n",
        "                                                    test_size = 0.2)\n",
        "# NOTE : uses df_normalized\n",
        "# NOTE : seed was set when optimizing models but I removed it after"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS3Pk1WQjH-K"
      },
      "source": [
        "Resampled (over-sampled) dataset\n",
        "\n",
        "*do not run if using cross validated dataset*\n",
        "\n",
        "*you can run on un-normalized and normalized datasets*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dVKlQqXXvMR"
      },
      "source": [
        "# over sampling data (only run if you want to over sample the data) !!!OPTIONAL!!!\n",
        "## CAUTION: have to do some variable editing if you want to use oversampled data\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_res, y_res = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "# NOTE : used for imbalanced dataset where you don't want to lose data to equally represent every target level (under sampling)\n",
        "# NOTE : use 'X_res' and  'y_res' to train model if you want to use oversampled data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIzh4gFoZRWr"
      },
      "source": [
        "# quick check too see if it did the oversampling \n",
        "print(np.count_nonzero(y_res == 1))\n",
        "print(np.count_nonzero(y_res == 2))\n",
        "print(np.count_nonzero(y_res == 3))\n",
        "print(np.count_nonzero(y_res == 4))\n",
        "print(np.count_nonzero(y_res == 5))\n",
        "print(np.count_nonzero(y_res == 6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPP-G2oTXA3i"
      },
      "source": [
        "# quick check of X and y before modeling to make sure we are using the intended dataset\n",
        "print(X_train)\n",
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RPSCPIrVxgN"
      },
      "source": [
        "# Modeling and evaluation\n",
        "\n",
        "*some parameter tuning was done for the below models (which I deleted in the final jupyter notebook) but i explained them in my documentation file.* \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlBfh2MnMI8d"
      },
      "source": [
        "Gradient boost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKpZHG6puNDQ"
      },
      "source": [
        "# XG Boost not CROSS VALIDATED\n",
        "classifier_XGBC = GradientBoostingClassifier()\n",
        "classifier_XGBC.fit(X_train, y_train)\n",
        "y_pred_XGBC = classifier_XGBC.predict(X_test)\n",
        "\n",
        "f1_scores_XGBC=f1_score(y_test, y_pred_XGBC, average=None)\n",
        "print(f1_scores_XGBC)\n",
        "mean(f1_scores_XGBC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMtjdwQHMzCZ"
      },
      "source": [
        "# XG Boost CROSS VALIDATED\n",
        "classifier_XGBC = GradientBoostingClassifier()\n",
        "kfold = KFold(n_splits=10)\n",
        "y_pred_XGBC = cross_val_predict(classifier_XGBC, X_no_split, y_no_split, cv=kfold) #Cross validation on training set\n",
        "\n",
        "f1_scores_XGBC=f1_score(y_no_split, y_pred_XGBC, average=None)\n",
        "mean(f1_scores_XGBC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gusFZE90ksL0"
      },
      "source": [
        "# XG Boost oversampled\n",
        "classifier_XGBC = GradientBoostingClassifier()\n",
        "classifier_XGBC.fit(X_train, y_train)\n",
        "y_pred_XGBC = classifier_XGBC.predict(X_test)\n",
        "\n",
        "f1_scores_XGBC=f1_score(y_test, y_pred_XGBC, average=None)\n",
        "print(f1_scores_XGBC)\n",
        "mean(f1_scores_XGBC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXMfIHhIjZK6"
      },
      "source": [
        "Decision tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbfHOXH4jbkK"
      },
      "source": [
        "# Decision tree NOT cross validated\n",
        "classifier_DTC = DecisionTreeClassifier()\n",
        "classifier_DTC.fit(X_train, y_train)\n",
        "y_pred_DTC = classifier_DTC.predict(X_test)\n",
        "\n",
        "f1_scores_DTC=f1_score(y_test, y_pred_DTC, average=None)\n",
        "print(f1_scores_DTC)\n",
        "mean(f1_scores_DTC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DztUd3rBkhOL"
      },
      "source": [
        "# Decision tree CROSS VALIDATED\n",
        "\n",
        "classifier_DTC = DecisionTreeClassifier()\n",
        "kfold = KFold(n_splits=10)\n",
        "y_pred_DTC = cross_val_predict(classifier_DTC, X_no_split, y_no_split, cv=kfold) #Cross validation on training set\n",
        "\n",
        "f1_scores_DTC=f1_score(y_no_split, y_pred_DTC, average=None)\n",
        "mean(f1_scores_DTC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW4D0xAlcL9V"
      },
      "source": [
        "# Decision tree OVER SAMPLED\n",
        "classifier_DTC = DecisionTreeClassifier()\n",
        "classifier_DTC.fit(X_res, y_res)\n",
        "y_pred_DTC = classifier_DTC.predict(X_test)\n",
        "\n",
        "f1_scores_DTC=f1_score(y_test, y_pred_DTC, average=None)\n",
        "print(f1_scores_DTC)\n",
        "mean(f1_scores_DTC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGctXCZcgg5a"
      },
      "source": [
        "Random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rGnG8NAgm-c"
      },
      "source": [
        "# Random forest NOT cross validated\n",
        "classifier_RFC = RandomForestClassifier()\n",
        "classifier_RFC.fit(X_train, y_train)\n",
        "y_pred_RFC = classifier_RFC.predict(X_test)\n",
        "\n",
        "f1_scores_RFC=f1_score(y_test, y_pred_RFC, average=None)\n",
        "print(f1_scores_RFC)\n",
        "mean(f1_scores_RFC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCdk64zYhbDF"
      },
      "source": [
        "# Random forest CROSS VALIDATED\n",
        "classifier_RFC = RandomForestClassifier()\n",
        "kfold = KFold(n_splits=10)\n",
        "y_pred_RFC = cross_val_predict(classifier_RFC, X_no_split, y_no_split, cv=kfold) #Cross validation on training set\n",
        "\n",
        "f1_scores_RFC=f1_score(y_no_split, y_pred_RFC, average=None)\n",
        "mean(f1_scores_RFC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko5gkyWGbeXH"
      },
      "source": [
        "# Random forest OVER SAMPLED\n",
        "classifier_RFC = RandomForestClassifier()\n",
        "classifier_RFC.fit(X_res, y_res)\n",
        "y_pred_RFC = classifier_RFC.predict(X_test)\n",
        "\n",
        "f1_scores_RFC=f1_score(y_test, y_pred_RFC, average=None)\n",
        "print(f1_scores_RFC)\n",
        "mean(f1_scores_RFC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OnngfdQmg4u"
      },
      "source": [
        "Adaboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e3W7EjQml8f"
      },
      "source": [
        "# Adaboost NOT cross validated\n",
        "classifier_ABC_RF = AdaBoostClassifier(RandomForestClassifier(), n_estimators= 30, learning_rate = 0.01) \n",
        "classifier_ABC_RF.fit(X_train, y_train)\n",
        "y_pred_ABC_RF = classifier_ABC_RF.predict(X_test)\n",
        "\n",
        "f1_scores_ABC=f1_score(y_test, y_pred_ABC_RF, average=None)\n",
        "mean(f1_scores_ABC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pte00xpum2vx"
      },
      "source": [
        "# Adaboost CROSS VALIDATED\n",
        "classifier_ABC_RF = AdaBoostClassifier(RandomForestClassifier(), learning_rate = 0.01) \n",
        "kfold = KFold(n_splits=10)\n",
        "y_pred_ABC_RF = cross_val_predict(classifier_ABC_RF, X_no_split, y_no_split, cv=kfold) #Cross validation on training set\n",
        "\n",
        "f1_scores_ABC=f1_score(y_no_split, y_pred_ABC_RF, average=None)\n",
        "mean(f1_scores_ABC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utXht7wwbvEW"
      },
      "source": [
        "# Adaboost OVER SAMPLED\n",
        "classifier_ABC_RF = AdaBoostClassifier(RandomForestClassifier(), n_estimators= 30, learning_rate = 0.01) \n",
        "classifier_ABC_RF.fit(X_res, y_res)\n",
        "y_pred_ABC_RF = classifier_ABC_RF.predict(X_test)\n",
        "\n",
        "f1_scores_ABC=f1_score(y_test, y_pred_ABC_RF, average=None)\n",
        "mean(f1_scores_ABC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkPm0JSGlxIQ"
      },
      "source": [
        "Neural net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwisWNyclAty"
      },
      "source": [
        "# MLP NOT cross validated\n",
        "classifier_neural_MLP = MLPClassifier(random_state=42)\n",
        "classifier_neural_MLP.fit(X_train, y_train)\n",
        "y_pred_MLP = classifier_neural_MLP.predict(X_test)\n",
        "\n",
        "f1_scores_MLP=f1_score(y_test, y_pred_MLP, average=None)\n",
        "mean(f1_scores_MLP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh1b3GyNmP2H"
      },
      "source": [
        "# MLP CROSS VALIDATED\n",
        "classifier_neural_MLP = MLPRegressor(random_state=42)\n",
        "kfold = KFold(n_splits=10)\n",
        "y_pred_MLP = cross_val_predict(classifier_neural_MLP, X_no_split, y_no_split, cv=kfold) #Cross validation on training set\n",
        "\n",
        "f1_scores_MLP=f1_score(y_no_split, y_pred_MLP, average=None)\n",
        "mean(f1_scores_MLP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQQyXjRyngbd"
      },
      "source": [
        "# MLP OVER SAMPLED\n",
        "classifier_neural_MLP = MLPClassifier(random_state=42)\n",
        "classifier_neural_MLP.fit(X_res, y_res)\n",
        "y_pred_MLP = classifier_neural_MLP.predict(X_test)\n",
        "\n",
        "f1_scores_MLP=f1_score(y_test, y_pred_MLP, average=None)\n",
        "mean(f1_scores_MLP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZozbiCBnVVm"
      },
      "source": [
        "Ensemble learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXnrtRlHocuC"
      },
      "source": [
        "# ensemble learning NOT cross validated\n",
        "\n",
        "voting_cl = VotingClassifier(estimators = [('Random Forest',classifier_RFC), ('Ada Boost',classifier_ABC_RF)], voting = 'hard')\n",
        "voting_cl.fit(X_train, y_train)\n",
        "y_pred_vcl = voting_cl.predict(X_test)\n",
        "\n",
        "\n",
        "f1_scores_Ensemble=f1_score(y_test, y_pred_vcl, average=None)\n",
        "mean(f1_scores_Ensemble)\n",
        "\n",
        "# NOTE: the different models used for ensemble learning changes dependeing on which models you want to try here. This is further discussed in the pdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiYD6_CBoqvS"
      },
      "source": [
        "# ensemble learning CROSS VALIDATED\n",
        "\n",
        "voting_cl = VotingClassifier(estimators = [('Decision Tree', classifier_DTC),\n",
        "                                              ('Random Forest',classifier_RFC),\n",
        "                                              ('Ada Boost',classifier_ABC_RF)], \n",
        "                                voting = 'hard')\n",
        "kfold = KFold(n_splits=10)\n",
        "y_pred_vcl = cross_val_predict(voting_cl, X_no_split, y_no_split, cv=kfold) # Cross validation on training set\n",
        "\n",
        "\n",
        "f1_scores_Ensemble=f1_score(y_no_split, y_pred_vcl, average=None)\n",
        "mean(f1_scores_Ensemble)\n",
        "\n",
        "# NOTE: the different models used for ensemble learning changes dependeing on which models you want to try here. This is further discussed in the pdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_bhUNa3cAoS"
      },
      "source": [
        "# Over sampled \n",
        "voting_cl = VotingClassifier(estimators = [('Random Forest',classifier_RFC), ('Ada Boost',classifier_ABC_RF)], voting = 'hard')\n",
        "voting_cl.fit(X_res, y_res)\n",
        "y_pred_vcl = voting_cl.predict(X_test)\n",
        "\n",
        "\n",
        "f1_scores_Ensemble=f1_score(y_test, y_pred_vcl, average=None)\n",
        "mean(f1_scores_Ensemble)\n",
        "\n",
        "# NOTE: the different models used for ensemble learning changes dependeing on which models you want to try here. This is further discussed in the pdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cqVGSAEo-Jt"
      },
      "source": [
        "# Feature evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yncS1jThpDZO"
      },
      "source": [
        "# determining which feature contribute most significantly to prediction\n",
        "importances = classifier_RFC.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in classifier_RFC.estimators_],\n",
        "             axis=0)\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Print the feature ranking\n",
        "print(\"Feature ranking:\")\n",
        "\n",
        "for f in range(X_nor.shape[1]):\n",
        "    print(\"%d. feature %d (%s) (%f)\" % (f + 1, indices[f], X_nor.columns[indices[f]], importances[indices[f]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGzw1umAuSCw"
      },
      "source": [
        "# Predict rating for test_new.csv to be sumbitted on Kaggle\n",
        "\n",
        "preparing X_test the same way as X_train\n",
        "\n",
        "different cells were ran to try different model variances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQV3993zU5NV"
      },
      "source": [
        "# loading the test_new.csv data as a pandas dataframe to be used later for kaggle prediction\n",
        "df_test_for_kaggle = pd.read_csv(\"/content/drive/My Drive/test_new.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPSEISK7IIxS"
      },
      "source": [
        "# peak into data\n",
        "\n",
        "# print first and last few rows of the dataset\n",
        "print(df_test_for_kaggle.head())\n",
        "print(df_test_for_kaggle.tail())\n",
        "\n",
        "# print all column headers + type\n",
        "print(df_test_for_kaggle.info())\n",
        "\n",
        "# checking for missing values for every feature\n",
        "df_test_for_kaggle.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aglgpCcjIVSe"
      },
      "source": [
        "# fixing some of the feature formatting for uniformity \n",
        "\n",
        "# fixing 'has_urgency_banner'\n",
        "# has_urgency_banner needs to be converted binomial so here I am filling 0 for the missing values\n",
        "df_test_for_kaggle['has_urgency_banner'] = df_test_for_kaggle['has_urgency_banner'].fillna(0)\n",
        "# has_urgency_banner needs to be converted binomial so here I am converting to int to match other binary columns\n",
        "df_test_for_kaggle.has_urgency_banner = df_test_for_kaggle.has_urgency_banner.astype(int)\n",
        "\n",
        "# fixing 'urgency_text'\n",
        "# has_urgency_banner needs to be converted binomial so here I am filling 0 for the missing values\n",
        "df_test_for_kaggle['urgency_text'] = df_test_for_kaggle['urgency_text'].fillna(0)\n",
        "# convert string to int to change uantité limitée ! to NaN\n",
        "df_test_for_kaggle.urgency_text = pd.to_numeric(df_test_for_kaggle.urgency_text, errors='coerce')\n",
        "# Nan to 1 to create binary field\n",
        "df_test_for_kaggle['urgency_text'] = df_test_for_kaggle['urgency_text'].fillna(1)\n",
        "# has_urgency_banner needs to be converted binomial so here I am converting to int to match other binary columns\n",
        "df_test_for_kaggle.urgency_text = df_test_for_kaggle.urgency_text.astype(int)\n",
        "\n",
        "# Since this feature consists mainly of US and China we can combine the rest into others\n",
        "df_test_for_kaggle['origin_country'] = df_test_for_kaggle['origin_country'].replace('VE', 'Other')\n",
        "df_test_for_kaggle['origin_country'] = df_test_for_kaggle['origin_country'].replace('AT', 'Other')\n",
        "df_test_for_kaggle['origin_country'] = df_test_for_kaggle['origin_country'].replace('SG', 'Other')\n",
        "df_test_for_kaggle['origin_country'] = df_test_for_kaggle['origin_country'].replace('GB', 'Other')\n",
        "df_test_for_kaggle['origin_country'] = df_test_for_kaggle['origin_country'].replace(np.nan, 'Other')\n",
        "# visual check to see if it correctly categorized everything\n",
        "sns.countplot('origin_country', data=df_test_for_kaggle)\n",
        "\n",
        "def below_ten(units_sold):\n",
        "    if units_sold < 10:\n",
        "        return 10\n",
        "    else:\n",
        "        return units_sold\n",
        "\n",
        "df_test_for_kaggle['units_sold'] = df_test_for_kaggle['units_sold'].apply(below_ten)\n",
        "# checking for distribution of units sold to see if it correctly categorized everything\n",
        "df_test_for_kaggle['units_sold'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbqaLB5VIwbS"
      },
      "source": [
        "# normalizing feature to prepare data for model prediction (for origin_country)\n",
        "df_test_for_kaggle = pd.get_dummies(df_test_for_kaggle, columns = ['origin_country'],\n",
        "                    prefix = 'country_')\n",
        "# quick check to see if it worked\n",
        "df_test_for_kaggle.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U08FYb3II23Z"
      },
      "source": [
        "# breaking up the tag column to see the number of tags per row. \n",
        "def tag_count(tags):\n",
        "    tag_str = tags\n",
        "    prod_tags = tag_str.split(',')\n",
        "    return len(prod_tags)\n",
        "    \n",
        "# replaced tags with tag_count from above to the df\n",
        "df_test_for_kaggle['tag_count'] = df_test_for_kaggle['tags'].apply(tag_count)\n",
        "df_test_for_kaggle.drop(['tags'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPu0bqkJ9rIj"
      },
      "source": [
        "# checking to see if difference in price between retail and price is associated with ratings\n",
        "df_test_for_kaggle['diff_in_price'] = round(df_test_for_kaggle['retail_price'] - df_test_for_kaggle['price'],2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK1tSD8wIqDS"
      },
      "source": [
        "# drop unneeded columns from dataset\n",
        "df_test_for_kaggle.drop(['currency_buyer', 'theme','crawl_month', 'shipping_option_name','inventory_total','merchant_name','merchant_title','merchant_info_subtitle','id','merchant_profile_picture','urgency_text','countries_shipped_to','product_color','product_variation_size_id','merchant_id','shipping_is_express','badge_local_product','badge_local_product'], axis=1, inplace=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggRbGYcCI-JZ"
      },
      "source": [
        "# CAUTION: only run if you want to normalize values\n",
        "min_max_scaler_kaggle = preprocessing.MinMaxScaler()\n",
        "x_scaled_kaggle = min_max_scaler_kaggle.fit_transform(df_test_for_kaggle)\n",
        "df_normalized_kaggle=pd.DataFrame(x_scaled_kaggle, columns=df_test_for_kaggle.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fJx1MRS9Fc8"
      },
      "source": [
        "# quick look\n",
        "print(df_normalized.info())\n",
        "print(df_normalized_kaggle.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_279rfO_JE2o"
      },
      "source": [
        "# name the set\n",
        "X_test_kaggle = df_normalized_kaggle\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IAjsYuFVTiP"
      },
      "source": [
        "# run this to create file with the id attached\n",
        "def gen_kaggle_submission(id,y_pred,filename=\"test_out.csv\"):\n",
        "  with open(\"/content/drive/My Drive/\"+filename,\"w\") as file_out:\n",
        "    file_out.write(\"id,rating\\n\")\n",
        "    for i in zip(id,y_pred):\n",
        "      str_out = f\"{i[0]},{i[1]:.1f}\\n\"\n",
        "      file_out.write(str_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skwuYAgUMN1u"
      },
      "source": [
        "# Random forest model output\n",
        "# run this cell for every difference variation of this model \n",
        "y_pred_kaggle=classifier_RFC.predict(X_test_kaggle)\n",
        "gen_kaggle_submission(test_id, y_pred_kaggle,\"rf.1_out.csv\")\n",
        "\n",
        "# NOTE : remember to change file name 'rf.1_out' for each output prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iApSeoOLQxEW"
      },
      "source": [
        "# ensemble learning model output\n",
        "# run this cell for every difference variation of this model \n",
        "y_pred_kaggle=voting_cl.predict(X_test_kaggle)\n",
        "gen_kaggle_submission(test_id, y_pred_kaggle,\"vc.1.1_out.csv\")\n",
        "\n",
        "# NOTE : remember to change file name 'vc.1_out' for each output prediction\n",
        "# NOTE : my kaggle submissions are all called vc because i at that point i was too lazy to change to file name for every variance of my models, but i obviously did the model just not the file name, my apologies :("
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnLCSdkCMOp3"
      },
      "source": [
        "# Adaboost model output\n",
        "# run this cell for every difference variation of this model \n",
        "y_pred_kaggle=classifier_ABC_RF.predict(X_test_kaggle)\n",
        "gen_kaggle_submission(test_id, y_pred_kaggle,\"ab.1_out.csv\")\n",
        "\n",
        "# NOTE : remember to change file name 'ab.1_out' for each output prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1KLE9qXA_26"
      },
      "source": [
        "# comparing the differences between the files. just to make sure 2 files aren't the exact same ya know\n",
        "!diff \"/content/drive/My Drive/vc.3_out.csv\" \"/content/drive/My Drive/vc.3_out-new.csv\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}